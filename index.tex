\documentclass[sigconf,authordraft]{acmart}
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

\usepackage{easy-todo}
\usepackage{listings}

\lstset{
  basicstyle=\ttfamily\footnotesize,
  frame=single,
}



\let\savedCite=\cite
\renewcommand{\cite}{\unskip~\savedCite}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmlicensed}
\copyrightyear{2018}
\acmYear{2018}
\acmDOI{XXXXXXX.XXXXXXX}
%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[Conference acronym 'XX]{Make sure to enter the correct
  conference title from your rights confirmation email}{June 03--05,
  2018}{Woodstock, NY}
\acmISBN{978-1-4503-XXXX-X/2018/06}



\begin{document}

\title{Barely Broken — Fuzzing with Almost Valid Inputs}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Valentin Huber}
\email{valentin.huber@cispa.de}
\orcid{1234-5678-9012}
\affiliation{%
    \institution{CISPA Helmholtz Center for Information Security}
    \city{}
    \country{Germany}
}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
% \renewcommand{\shortauthors}{Trovato et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
    \todo{write me}
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
    <ccs2012>
    <concept>
    <concept_id>00000000.0000000.0000000</concept_id>
    <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
    <concept_significance>500</concept_significance>
    </concept>
    <concept>
    <concept_id>00000000.00000000.00000000</concept_id>
    <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
    <concept_significance>300</concept_significance>
    </concept>
    <concept>
    <concept_id>00000000.00000000.00000000</concept_id>
    <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
    <concept_significance>100</concept_significance>
    </concept>
    <concept>
    <concept_id>00000000.00000000.00000000</concept_id>
    <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
    <concept_significance>100</concept_significance>
    </concept>
    </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Do Not Use This Code~Generate the Correct Terms for Your Paper}
\ccsdesc[300]{Do Not Use This Code~Generate the Correct Terms for Your Paper}
\ccsdesc{Do Not Use This Code~Generate the Correct Terms for Your Paper}
\ccsdesc[100]{Do Not Use This Code~Generate the Correct Terms for Your Paper}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Fuzzing, Grammar Fuzzing, Language Fuzzing}

\received{20 February 2007}
\received[revised]{12 March 2009}
\received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\todo{change title}

\section{Introduction}

\begin{figure}
    \begin{lstlisting}
VBq-"7.6arK w;zu%6$K>%OTV"ryD*
    \end{lstlisting}
    \begin{lstlisting}
&& float += % " ( ; == [
    \end{lstlisting}
    \begin{lstlisting}
int main() { return i; }
    \end{lstlisting}
    \begin{lstlisting}
int main() { return 0; }
    \end{lstlisting}
    \caption{Inputs for a C compiler that get rejected by the lexer, are syntactically and semantically invalid, and are fully valid, respectively.}
    \Description{Inputs for a C compiler that get rejected by the lexer, are syntactically and semantically invalid, and are fully valid, respectively.}
    \label{fig:c_examples}
\end{figure}

Automated test generators such as fuzzers have been proven to be highly effective at finding software errors along these steps. However, not all test generators provide inputs that test the same parts of the system under test (SUT).\cite{Demystifying}

Program behavior under a certain input shows similarities observed in a wide range of programs. Program input is typically provided as unstructured data (such as a binary blob containing data for an image). This input is first parsed into data structures that can then be processed by the program's business logic. For an input to be fully processed by a SUT, the linearized data in this input needs to adhere to an expected input structure, i. e. it needs to be parseable and pass all input validation checks.

Different programs have vastly different requirements for their inputs: zip is designed to process any data during compression, while compilers need to be very strict about the input they allow. Generally, one can distinguish between three steps in input validation, correlating to the examples given in Figure~\ref{fig:c_examples}. They are increasingly less likely to be fulfilled by random data:

\begin{enumerate}
    \item \textbf{Lexing}: Random bytes are unlikely to produce exclusively valid input tokens, such as valid keywords, operators, or variable names. The rules for this steps can be given as a list of valid input tokens.
    \item \textbf{Syntax}: Even if one would provide a random list of exclusively valid tokens, they likely do not appear in the right structures. The rules for this step are typically given as a context-free grammar.
    \item \textbf{Semantics}: Programming languages require many additional checks, such as that each variable is defined before it is used. Recent works propose giving such additional constraints as code evaluated on elements of the grammar\cite{Fandango}.
\end{enumerate}

Various approaches have been suggested to produce inputs that adhere to some or all of these constraints\cite{AFLPlusPlus,Nautilus,Fandango,Zest,Autarkie,FrameShift,RedQueen,GrammarMutation}.

In this work, I present the following contributions:
\begin{enumerate}
    \item I propose two alternative ways of \textit{evaluating test generators} by examining the inputs they produce for \textit{correctness} and \textit{interestingness}, along with proposals for various approaches to measuring them.
    \item I argue why I suspect that significant portions of the input space of SUTs requiring highly structured inputs remain untested, along with results from initial experiments confirming my suspicion.
    \item Finally, I provide three suggested approaches to closing this gap.
\end{enumerate}

I explore how to evaluate suspected limitations in these approaches for testing SUTs expecting highly structured inputs, along with potential solutions.

\todo{Talk about prior research somewhere, even if unpublished?}

\section{Evaluating Test Input Quality}

\label{sec:measurements}

Evaluating test generators can be done in multiple, orthogonal ways. The most common measurement is the code coverage reached when executing a certain input on an instrumented implementation of the SUT. But while code coverage is comparatively easy to measure due to the availability of instrumentation passes in popular compilers\cite{gcov,SanCov}\todo{citation format of these} and binary-only fuzzing tools\cite{LibAFLQEMU}, it has two fundamental limitations: (1) It assigns equal weight to all branches and (2) assumes any value covering a new branch is equally interesting.

While code coverage is often evaluated dynamically during a fuzzing campaign to aid the fuzzer in progressing through a program, this has to be distinguished from using code coverage as a technique to evaluate fuzzer performance.

\subsection{Input Correctness}

Limitation (1) in evaluating a fuzzer's performance means that fuzzers that reach far into the SUT but only provide valid inputs may receive the same score as a fuzzer that tests all error paths in the initial input validation steps of a program. Alternatively, I propose evaluating fuzzers for \textit{input correctness}. The correctness of an input with regards to a SUT can be evaluated in two ways:

\begin{enumerate}
    \item First, one can test inputs on an implementation of a SUT, measuring the ratio of accepted to rejected inputs. This can further be refined by annotating or instrumenting the code to record the step at which a certain input is rejected. This can be achieved with one of the following:
          \begin{enumerate}
              \item Manual annotations of steps along the path through the program, similar to \cite{FuzzFactory,IJON}
              \item Selective coverage instrumentation of entire program parts as a very coarse measurement\cite{Zest}
              \item I further propose the following heuristic: Based on a diverse set of (fuzzer-produced) inputs, \textit{instrument the edges executed by all valid inputs}.
          \end{enumerate}
    \item Alternatively, inputs can be evaluated against an abstract specification of the language expected by a SUT. Such specifications are usually given in natural language, but recent works have explored how to express such specification in a structured way, as a combination of a context-free grammar and additional constraints over nodes in this grammar\cite{Fandango}. Evaluating the correctness against such a specification may be done in different steps:
          \begin{enumerate}
              \item For inputs that cannot be parsed with the provided grammar, existing literature provides algorithm to measure how close to valid the input is.\cite{DistanceInitial, DistanceLessThanCubic}
              \item For inputs that are parsed, the ratio of fulfilled to violated constraints can be used to assess the correctness of an input.
          \end{enumerate}
\end{enumerate}

\subsection{Input Interestingness}

Not all inputs are equally interesting, even if they are all fully correct. According to the testing principle of equivalence classes, incrementing a byte is unlikely to trigger a bug. This is of course unless this new byte is at the edge of legality for the SUT, for example by changing this byte so that it is no longer in a valid range. This is known as boundary testing: If a requirement allows values between 5 and 100, the likelihood of discovering a problem is higher with values 4, 5, 6, 99, 100, and 101 compared to values between and outside. Special values such as 0, -1, or the maximum possible number may further be interesting.

Previously, it was non-trivial to evaluate how close an input part is to such a constraint boundary. With the availability of formal and machine-readable specifications, such as described above, this now becomes possible. An input (part) is more interesting, the closer to the boundaries provided by the specification it is. This can be applied to semantic constraints, but also syntactic structure: If a node can be repeated an arbitrary number of times, testing 0 repetitions, 1 repetition and a large number of repetitions is interesting. This can be reinterpreted as syntactic boundary testing.

\section{Evaluating Existing Test Generators}

I do not know of any work systematically evaluating correctness and interestingness of the inputs produced by different test generators. Considering that previous work found significant problems with the evaluation of fuzzers\cite{SOKFuzzingEvaluation}, evaluating different approaches along the axes outlined in Section~\ref{sec:measurements} may lead to greater insight into what approaches are actually evaluating interesting parts of the target; or more importantly, which approaches produce individual in parts of the input space no other test generator reaches.

For an initial experiment, I ran the grammar fuzzer Nautilus\footnote{Nautilus version 2.0 through libafl\_nautilus; which notably no longer provides out-of-grammar fuzzing}\cite{Nautilus} against the JavaScript runtime QuickJS and the C compiler clang with respective grammars. According to the first correctness metric outlined above, I recorded the correctness of inputs produced by the fuzzer.

Running against QuickJS, Nautilus produced approximately 45\% inputs that could not be parsed, 5\% inputs that resulted in an exception, and 50\% inputs that were evaluated without an error. For clang, the results show considerable differences: Only about 50\% of inputs made it past the lexing step, and not a single one made it past the parser. One can therefore see how the compiler, assembler, and linker step of clang are never tested with this fuzzing setup. This is likely because clang uses custom lexing and parsing logic that integrates semantic checks. So while the inputs produced by Nautilus are discarded by clang, they by construction can be parsed by the abstract grammar.

The differences in complexity of constraints between JavaScript and C do not explain the difference in result. It seems much more likely that these differences are because of the permissiveness and dynamic typing of JavaScript, while many deeper parts of the interpreter are not actually getting tested. Here, evaluating test correctness against a specification may be more appropriate.

For clang, Nautilus does not seem to be able to generate test inputs that fully compile and thus test all program parts; other approaches should be used, at least in addition to Nautilus.

\todo{Add plot of correctness levels against time}

\section{The Ladder to Success: Categorizing Approaches}

One can categorize most fuzzers along the correctness of the inputs they provide. Imagine the distribution of bugs like the distribution of berries in a bush, with berries higher up in the bush representing more correct inputs, as shown in Figure~\ref{fig:bush}. Historically, a set of fundamentally different approaches was proposed. These produce inputs that have varying distributions of input correctness and interestingness and therefore test different parts of the program and check for different kinds of software errors. The following section provides a categorization of approaches along with the program parts they test, as indicated in Figure~\ref{fig:bush}. The exact extent of the figurative reach of each fuzzer remains unknown; this figure is not to scale, including the amount of overlap.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\linewidth]{bush}
    \caption{Bush representing the input space of a program, where increasing height represents increasing correctness of a certain input}
    \Description{Drawing of a bush with a ladder and overlayed numbered ellipses. The bush represents the input space of a system under test, with increasing height in the bush representing increasing correctness of a certain input.}
    \label{fig:bush}
\end{figure}

The initial approach was using fully random data\cite{UNIX}. As discussed above, this will only test the outermost input validation, equivalent to the very low-hanging fruit (as represented by the ellipsis labeled I).

The next major step was the introduction of naïve mutational coverage-guided fuzzing (CGF) in American Fuzzy Lop (AFL)\cite{AFL}. By reaching more and more coverage, these fuzzers "learn" the structure the inputs they produce require to reach more coverage. They are however severely limited by the kind of structure they can produce: Even multi-byte comparison operations requires them to guess the value. These fuzzers reached more of the low-hanging fruit in the tree of bugs with no help in the form of an input specification from the analyst (II).

Some of these limitations were mitigated to a point by extending CGF with additional instrumentation. By logging the values input parts are compared to, the fuzzer can mutate the input again to fix these parts and therefore pass the checks \cite{RedQueen}. This further allows CGF to pass simple checksums. More recent advancements allow CGF to automatically fix length fields based on sudden coverage drop-offs\cite{FrameShift}. These improvements allow the fuzzer to produce more correct inputs and reach further up the tree of bugs (III).

However, certain structure sill cannot be learned by fuzzers from feedback, which is why\todo{cite} grammar fuzzers were introduced\cite{Nautilus}. These start from a different place, producing inputs that pass at least the lexing and parsing steps of a SUT\footnote{At least in principle; unless constraint validation is intermixed with parsing}. By providing the grammar, the analyst "helps" the fuzzer onto the first step of a ladder on the bush where it reaches left and right (IV).

From this next step on the ladder, fuzzers can reach further down by producing inputs that do not (fully) adhere to the grammar, by mutating the grammar or the trees after production (V)\cite{GrammarMutation,Nautilus}. There have been initial steps towards extending grammar-based fuzzers to reach further up, for example by using the same instrumentation on value comparisons as described above (VI)\cite{Autarkie}.

Language-based fuzzing extends context-free grammars with additional constraints that ensure the inputs produced are not only syntactically but also semantically correct. This correlates to an analyst providing all information necessary for the fuzzer to produce fully correct input (assuming a complete and correct specification), which is equivalent to the topmost step of the ladder (VII).

Fuzzers based on symbolic execution can be thought of as systematically walking each branch of the bush. In some of my previous work, I have discussed the limitations of fuzzers relying on symbolic execution\cite{SymbexSurvey}.

There further exists a list of fuzzers purpose-built for a certain target specification. They contain information about the expected input format, encoded in their logic. And while they may be highly optimized, they are fundamentally equivalent to a generic fuzzer given the same information in the form of grammar or language specifications and should be categorized accordingly.

\section{Reaching the Rest of the Tree}

The combined input space coverage as seen in Figure~\ref{fig:bush} seems to suggest that existing approaches may not reach all program logic; although this will have to be validated with a survey based on the measurements as described in Section~\ref{sec:measurements}. The following approaches may do so:
\begin{enumerate}
    \item \textbf{Reach down from the top}: Similarly to out-of-grammar fuzzing, I want to explore out-of-language fuzzing, where constraints of a full specification are iteratively and deliberately violated, while the remaining constraints are still fulfilled. This approach could further be improved by out-of-grammar construction of the inputs that are evaluated for constraints.
    \item \textbf{Incremental steps towards the top}: Writing fully correct specifications for tools like Fandango is considerable work due to a current lack of automated specification generation. However, providing a few simple constraints to an existing grammar may be enough to allow a fuzzer to reach code it was previously unable to test. This by design will additionally produce inputs that do not match the full specification, thus leading to a similar effect as approach (1).
    \item \textbf{Targeted testing}: Due to the availability of tools able to use full input specifications, I would like to explore fuzzers producing inputs that reach higher interestingness in their test cases by steering input generation towards boundaries of both the input semantics (i.e. the constraints) and the syntax (the structure).
\end{enumerate}

\section{Conclusion}

In this work, I outline how suspected limitations of existing test generators; namely their inability to test parts of target software that requires highly structured inputs to be reached. Initial results on a JavaScript interpreter and a C compiler suggest significant parts of such targets may not be tested at all by current test generators.

I further propose multiple measurements to evaluate the inputs of existing and future test generators with regards to their correctness against an implementation and an abstract specification, along with their interestingness as measured by distance to boundaries of equivalence classes.

Finally, I propose multiple approaches of how to design future test generators to fill gaps in existing work, agnostic to specification and implementation.

% \cleardoublepage

% \section{}

% Automated test generation, such as fuzzing, has proven to be an effective strategy for finding bugs and vulnerabilities in a wide range of software and hardware. In their initial work on fuzzing, \citeauthor{UNIX} used entirely unstructured random data\cite{UNIX}. And while this approach lead to a significant list of bugs, for any program expecting its input to adhere to a certain structure and performing trivial input validation, it is highly unlikely to execute any of the program logic besides these outermost input validation checks. And while these are extensively tested by such input generation techniques, most logic and therefore potential bugs are shielded from ever being executed and thus tested.

% To increase the parts of a certain system under test (SUT) that are actually evaluated by a test generator system, this system needs to ensure that the structure of the inputs it produces adheres (at least partially) to the structure expected by the SUT.


% \section{Related Work: Categorization of Approaches}
% \subsection{Base Case: Unstructured}
% \begin{itemize}
%   \item Pure Randomness, see \cite{UNIX}
%   \item Only short, refer to introduction
% \end{itemize}
% \subsection{Specialized Fuzzers}
% \begin{itemize}
%   \item Successful, but not generalizable
%   \item Major code duplication across implementations
%   \item Cite MT
%   \item Keep short, later refer back to this from language based fuzzing as a generic version of this
% \end{itemize}
% \subsection{Incrementally Learning Input Structure: Coverage Guided Fuzzing}
% \begin{itemize}
%   \item Why this learns structure
%   \item Upside: No domain specific knowledge necessary
%   \item Downside: Trivial data-interdependencies cannot be passed (length fields, checksums)
% \end{itemize}
% \subsection{Automatically Fixing Input Structure: Extensions on Greybox Fuzzing}
% \begin{itemize}
%   \item cmplog/RedQueen
%   \item Automatic length field detection
%   \item Still upside of little domain specific knowledge necessary
%   \item Surprisingly effective
%   \item Ineffective at more complex (nested) constraints (XML, programming languages, etc.)
% \end{itemize}
% \subsection{Systematically Exploring Structure: Symbolic Execution}
% \begin{itemize}
%   \item In principle: Tests all parts of the SUT, by construction and effectively (assurances about completeness)
%   \item In practice: Doesn't scale, limitations on environment interaction etc.
%   \item Concolic testing as a middle ground, still significant limitations
% \end{itemize}
% \subsection{Enforcing Structure by Construction: Grammar Fuzzing}
% \begin{itemize}
%   \item Downside: Manually specify structure
%   \item Upside: Manually specify structure, not reliant on heuristics that struggle with weird edge cases
%   \item Still limitations for constraints that cannot be expressed in context free grammars
% \end{itemize}
% \subsection{Expanding Grammar Fuzzers: Out of Grammar Testing}
% \begin{itemize}
%   \item Shadowed subtrees \cite{Nautilus}
%   \item Mutating grammars \cite{GrammarMutation}
%   \item OOG inputs as seed for greybox fuzzer: Nautilus (partial)
% \end{itemize}
% \subsection{Producing Fully Valid Inputs: Language Fuzzing}
% \begin{itemize}
%   \item Refer to custom fuzzers
%   \item One step towards the more abstract: Custom Unparsing in Nautilus\cite{Nautilus}
%   \item FANDANGO\cite{Fandango}
% \end{itemize}


% \section{ToThink}
% \begin{itemize}
%   \item Another axis: Abstract config vs. code: Custom fuzzers are all code. Fandango is all config. Nautilus with custom unparsing is half/half. Code is easier(?), but abstract is more flexible, as improvements scale across all configs, and as flexible as code.
% \end{itemize}

% \section{Target: Where Bugs Are}
% \begin{itemize}
%   \item Previously: Everywhere
%   \item Now: In untested parts
%   \item Fundamentally: In barely valid inputs (off by 1 etc.), \textit{just} off the beaten (happy) path \todo{Examples?}
%   \item Understand what fully correct inputs are: Semantic, Syntactic, Lexical
%   \item State?
%   \item Understand which parts are actually tested \todo{Argument just based on principle? Short evaluation?}
% \end{itemize}

% \section{Method: How to Test These}
% \begin{itemize}
%   \item Out of Constraint Testing
%   \item More general: Out of language at all levels (semantic, syntactic, lexical)
%   \item Combined
% \end{itemize}

% \section{Evaluation}
% \begin{itemize}
%   \item Differential Coverage (OOL doesn't have to do everything better, ensembling is easy)
% \end{itemize}

% \cleardoublepage
% \begin{itemize}
%   \item How interesting are inputs from existing test generators for highly structured inputs? Create testbench.
%         \begin{itemize}
%           \item Along the program run (lexing, parsing, input validation, etc.) with instrumentation
%           \item Against specification (lexing, parsing, constraints) with Fandango parse
%           \item Interesting values (edge of constraint, min/max of grammar, etc.) with Fandango
%         \end{itemize}
%   \item Create test generators that fill holes. For example:
%         \begin{itemize}
%           \item Fandango out of language (reach down from fully correct input)
%           \item Fandango towards boundaries of spec
%           \item Fandango with partial spec (reach up from the next step past grammar fuzzers)
%         \end{itemize}
% \end{itemize}


% \cleardoublepage



%%
%% The acknowledgments section is defined using the "acks" environment
%% (and NOT an unnumbered section). This ensures the proper
%% identification of the section in the article metadata, and the
%% consistent spelling of the heading.
% \begin{acks}
%     To Robert, for the bagels and explaining CMYK and color spaces.
% \end{acks}

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{sources.bib}


%%
%% If your work has an appendix, this is the place to put it.
% \appendix

% \section{Research Methods}

\end{document}
\endinput
%%
%% End of file `sample-sigconf-authordraft.tex'.